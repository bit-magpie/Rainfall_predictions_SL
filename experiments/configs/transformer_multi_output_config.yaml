model:
  name: "transformer_multi_output"
  params:
    d_model: 64       # Embedding dimension
    nhead: 4          # Number of attention heads
    num_layers: 3     # Number of transformer encoder layers
    dim_feedforward: 256  # Dimension of feedforward network
    output_size: 3    # Three outputs: rainfall, min temp, max temp
    dropout: 0.1

data:
  file_path: "data/combined_data.csv"
  target_cols: ["Rainfall", "Min_Temperature", "Max_Temperature"]
  seq_length: 24      # Longer sequence for transformer to leverage attention mechanism
  test_size: 0.15
  val_size: 0.15
  random_state: 42
  batch_size: 32

training:
  epochs: 300
  learning_rate: 0.0005  # Lower learning rate for transformer
  weight_decay: 0.0001
  max_grad_norm: 5.0

paths:
  model_save_path: "results/models/transformer_multi_output_model.pt"
  metrics_save_path: "results/metrics/transformer_multi_output_metrics.csv"
  figures_path: "results/figures/transformer_multi_output/"