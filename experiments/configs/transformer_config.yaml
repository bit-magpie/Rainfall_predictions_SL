model:
  name: "transformer"
  params:
    d_model: 64       # Embedding dimension
    nhead: 4          # Number of attention heads
    num_layers: 2     # Number of transformer encoder layers
    dim_feedforward: 256  # Dimension of feedforward network
    output_size: 1
    dropout: 0.1

data:
  file_path: "data/combined_data.csv"
  target_col: "Rainfall"
  seq_length: 24      # Longer sequence for transformer to leverage attention over longer time periods
  test_size: 0.2
  val_size: 0.2
  random_state: 42
  batch_size: 32

training:
  epochs: 300
  learning_rate: 0.0005  # Lower learning rate for transformer
  weight_decay: 0.0001
  max_grad_norm: 5.0

paths:
  model_save_path: "results/models/transformer_model.pt"
  metrics_save_path: "results/metrics/transformer_metrics.csv"
  figures_path: "results/figures/transformer/"